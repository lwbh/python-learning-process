

```python
# -*- coding: utf-8 -*-
#12.23作业
#urllib.request中两个内置函数的用法
import urllib.request
dir(urllib.request)
```




    ['AbstractBasicAuthHandler',
     'AbstractDigestAuthHandler',
     'AbstractHTTPHandler',
     'BaseHandler',
     'CacheFTPHandler',
     'ContentTooShortError',
     'DataHandler',
     'FTPHandler',
     'FancyURLopener',
     'FileHandler',
     'HTTPBasicAuthHandler',
     'HTTPCookieProcessor',
     'HTTPDefaultErrorHandler',
     'HTTPDigestAuthHandler',
     'HTTPError',
     'HTTPErrorProcessor',
     'HTTPHandler',
     'HTTPPasswordMgr',
     'HTTPPasswordMgrWithDefaultRealm',
     'HTTPPasswordMgrWithPriorAuth',
     'HTTPRedirectHandler',
     'HTTPSHandler',
     'MAXFTPCACHE',
     'OpenerDirector',
     'ProxyBasicAuthHandler',
     'ProxyDigestAuthHandler',
     'ProxyHandler',
     'Request',
     'URLError',
     'URLopener',
     'UnknownHandler',
     '__all__',
     '__builtins__',
     '__cached__',
     '__doc__',
     '__file__',
     '__loader__',
     '__name__',
     '__package__',
     '__spec__',
     '__version__',
     '_cut_port_re',
     '_ftperrors',
     '_have_ssl',
     '_localhost',
     '_noheaders',
     '_opener',
     '_parse_proxy',
     '_proxy_bypass_macosx_sysconf',
     '_randombytes',
     '_safe_gethostbyname',
     '_thishost',
     '_url_tempfiles',
     'addclosehook',
     'addinfourl',
     'base64',
     'bisect',
     'build_opener',
     'contextlib',
     'email',
     'ftpcache',
     'ftperrors',
     'ftpwrapper',
     'getproxies',
     'getproxies_environment',
     'getproxies_registry',
     'hashlib',
     'http',
     'install_opener',
     'io',
     'localhost',
     'noheaders',
     'os',
     'parse_http_list',
     'parse_keqv_list',
     'pathname2url',
     'posixpath',
     'proxy_bypass',
     'proxy_bypass_environment',
     'proxy_bypass_registry',
     'quote',
     're',
     'request_host',
     'socket',
     'splitattr',
     'splithost',
     'splitpasswd',
     'splitport',
     'splitquery',
     'splittag',
     'splittype',
     'splituser',
     'splitvalue',
     'ssl',
     'string',
     'sys',
     'tempfile',
     'thishost',
     'time',
     'to_bytes',
     'unquote',
     'unquote_to_bytes',
     'unwrap',
     'url2pathname',
     'urlcleanup',
     'urljoin',
     'urlopen',
     'urlparse',
     'urlretrieve',
     'urlsplit',
     'urlunparse',
     'warnings']




```python
urllib.request.urlopen('https://www.baidu.com/')
```




    <http.client.HTTPResponse at 0x2c140e5cdd8>




```python
import urllib.request
request=urllib.request.Request('https://www.baidu.com')
reponse=urllib.request.urlopen(request)
print(reponse.read().decode('utf-8'))
print(type(reponse))
```

    <html>
    <head>
    	<script>
    		location.replace(location.href.replace("https://","http://"));
    	</script>
    </head>
    <body>
    	<noscript><meta http-equiv="refresh" content="0;url=http://www.baidu.com/"></noscript>
    </body>
    </html>
    <class 'http.client.HTTPResponse'>
    


```python
#BeautifulSoup的应用
from bs4 import BeautifulSoup
dir(BeautifulSoup)
```




    ['ASCII_SPACES',
     'DEFAULT_BUILDER_FEATURES',
     'HTML_FORMATTERS',
     'NO_PARSER_SPECIFIED_WARNING',
     'ROOT_TAG_NAME',
     'XML_FORMATTERS',
     '__bool__',
     '__call__',
     '__class__',
     '__contains__',
     '__copy__',
     '__delattr__',
     '__delitem__',
     '__dict__',
     '__dir__',
     '__doc__',
     '__eq__',
     '__format__',
     '__ge__',
     '__getattr__',
     '__getattribute__',
     '__getitem__',
     '__getstate__',
     '__gt__',
     '__hash__',
     '__init__',
     '__init_subclass__',
     '__iter__',
     '__le__',
     '__len__',
     '__lt__',
     '__module__',
     '__ne__',
     '__new__',
     '__reduce__',
     '__reduce_ex__',
     '__repr__',
     '__setattr__',
     '__setitem__',
     '__sizeof__',
     '__str__',
     '__subclasshook__',
     '__unicode__',
     '__weakref__',
     '_all_strings',
     '_attr_value_as_string',
     '_attribute_checker',
     '_check_markup_is_url',
     '_feed',
     '_find_all',
     '_find_one',
     '_formatter_for_name',
     '_is_xml',
     '_lastRecursiveChild',
     '_last_descendant',
     '_popToTag',
     '_select_debug',
     '_selector_combinators',
     '_should_pretty_print',
     '_tag_name_matches_and',
     'append',
     'attribselect_re',
     'childGenerator',
     'children',
     'clear',
     'decode',
     'decode_contents',
     'decompose',
     'descendants',
     'encode',
     'encode_contents',
     'endData',
     'extract',
     'fetchNextSiblings',
     'fetchParents',
     'fetchPrevious',
     'fetchPreviousSiblings',
     'find',
     'findAll',
     'findAllNext',
     'findAllPrevious',
     'findChild',
     'findChildren',
     'findNext',
     'findNextSibling',
     'findNextSiblings',
     'findParent',
     'findParents',
     'findPrevious',
     'findPreviousSibling',
     'findPreviousSiblings',
     'find_all',
     'find_all_next',
     'find_all_previous',
     'find_next',
     'find_next_sibling',
     'find_next_siblings',
     'find_parent',
     'find_parents',
     'find_previous',
     'find_previous_sibling',
     'find_previous_siblings',
     'format_string',
     'get',
     'getText',
     'get_attribute_list',
     'get_text',
     'handle_data',
     'handle_endtag',
     'handle_starttag',
     'has_attr',
     'has_key',
     'index',
     'insert',
     'insert_after',
     'insert_before',
     'isSelfClosing',
     'is_empty_element',
     'new_string',
     'new_tag',
     'next',
     'nextGenerator',
     'nextSibling',
     'nextSiblingGenerator',
     'next_elements',
     'next_siblings',
     'object_was_parsed',
     'parentGenerator',
     'parents',
     'parserClass',
     'popTag',
     'prettify',
     'previous',
     'previousGenerator',
     'previousSibling',
     'previousSiblingGenerator',
     'previous_elements',
     'previous_siblings',
     'pushTag',
     'quoted_colon',
     'recursiveChildGenerator',
     'renderContents',
     'replaceWith',
     'replaceWithChildren',
     'replace_with',
     'replace_with_children',
     'reset',
     'select',
     'select_one',
     'setup',
     'string',
     'strings',
     'stripped_strings',
     'tag_name_re',
     'text',
     'unwrap',
     'wrap']




```python
from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""
soup=BeautifulSoup(html_doc)
print(soup.prettify)
```

    <bound method Tag.prettify of <html><head><title>The Dormouse's story</title></head>
    <body>
    <p class="title"><b>The Dormouse's story</b></p>
    <p class="story">Once upon a time there were three little sisters; and their names were
    <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
    <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and
    <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;
    and they lived at the bottom of a well.</p>
    <p class="story">...</p>
    </body></html>>
    


```python
soup.title
soup.title.name
soup.find_all('a')

```




    [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
     <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
     <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]




```python
for link in soup.find_all('a'):
    print(link.get('href'))
```

    http://example.com/elsie
    http://example.com/lacie
    http://example.com/tillie
    


```python
print(soup.get_text())
```

    The Dormouse's story
    
    The Dormouse's story
    Once upon a time there were three little sisters; and their names were
    Elsie,
    Lacie and
    Tillie;
    and they lived at the bottom of a well.
    ...
    
    


```python
#天气预报网站爬虫
import requests

from bs4 import BeautifulSoup


url = 'http://www.weather.com.cn/weather/101010100.shtml'
response = requests.get(url)
content = response.content
html_doc = content.decode(encoding = 'utf-8')

soup = BeautifulSoup(html_doc)
ul = soup.find_all("ul", "t clearfix")[0]
lis = ul.find_all("li")
for li in lis:
    li_1 = li.get_text().replace('\n', ' ')
    li_2 = li_1.replace('℃', '℃ ')
    print(li_2)
```

     26日（今天）   晴  -7℃       <3级   
     27日（明天）   晴  4℃ /-7℃        <3级   
     28日（后天）   晴转多云  4℃ /-6℃        <3级   
     29日（周日）   晴  6℃ /-8℃        4-5级   
     30日（周一）   晴  -5℃ /-11℃        4-5级转<3级   
     31日（周二）   晴  -2℃ /-11℃        3-4级转<3级   
     1日（周三）   晴  1℃ /-10℃        <3级   
    


```python

```
